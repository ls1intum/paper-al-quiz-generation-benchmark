benchmark:
  name: "example-benchmark"
  version: "1.0.0"
  runs: 3  # Number of times to repeat the evaluation

evaluators:
  gpt4:
    provider: "azure_openai"
    model: "gpt-4"  # Your Azure deployment name
    temperature: 0.0
    max_tokens: 500

  gpt35:
    provider: "azure_openai"
    model: "gpt-35-turbo"  # Your Azure deployment name
    temperature: 0.0
    max_tokens: 500

  claude_opus:
    provider: "anthropic"
    model: "claude-3-opus-20240229"
    temperature: 0.0
    max_tokens: 500

  claude_sonnet:
    provider: "anthropic"
    model: "claude-3-5-sonnet-20241022"
    temperature: 0.0
    max_tokens: 500

metrics:
  - name: "difficulty"
    version: "1.0"
    evaluators: ["gpt4", "gpt35"]  # Compare GPT-4 vs GPT-3.5
    parameters:
      rubric: "bloom_taxonomy"
      target_audience: "undergraduate"
    enabled: false

  - name: "coverage"
    version: "1.0"
    evaluators: ["claude_opus"]
    parameters:
      granularity: "balanced"
    enabled: false

  - name: "grammatical_correctness"
    version: "1.0"
    evaluators: ["gpt4"]
    enabled: true

  - name: "clarity"
    version: "1.0"
    evaluators: ["gpt4", "claude_sonnet"]  # Compare different models
    enabled: false

inputs:
  quiz_directory: "data/quizzes"
  source_directory: "data/inputs"

outputs:
  results_directory: "data/results"
  aggregate: true
