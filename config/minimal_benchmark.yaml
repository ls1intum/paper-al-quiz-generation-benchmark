benchmark:
  name: "minimal-benchmark"
  version: "1.0.0"
  runs: 1  # Number of times to repeat the evaluation

evaluators:
  gpt5:
    provider: "azure_openai"
    model: "gpt-5"  # Your Azure deployment name
    temperature: 0.0
    max_tokens: 1000

  gpt4:
    provider: "azure_openai"
    model: "gpt-4o"
    temperature: 0.0
    max_tokens: 1500

metrics:
  - name: "coverage"
    version: "1.1"
    evaluators: [ "gpt4" ]
    enabled: true

inputs:
  quiz_directory: "data/quizzes"
  source_directory: "data/inputs"

outputs:
  results_directory: "data/results"
  aggregate: true
