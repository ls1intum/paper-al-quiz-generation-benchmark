benchmark:
  name: "example-benchmark"
  version: "1.0.0"
  runs: 3  # Number of times to repeat the evaluation

evaluators:
  azure_gpt4:
    provider: "azure_openai"
    model: "gpt-4"  # Your Azure deployment name
    temperature: 0.0
    max_tokens: 500

  lmstudio_fast:
    provider: "lm_studio"
    model: "qwen2.5-7b-instruct"  # Example local model
    base_url: "http://localhost:1234/v1"  # LM Studio OpenAI endpoint
    temperature: 0.0
    max_tokens: 300

  lmstudio_reasoning:
    provider: "lm_studio"
    model: "qwen2.5-14b-instruct"  # Example local model
    base_url: "http://localhost:1234/v1"  # LM Studio OpenAI endpoint
    temperature: 0.0
    max_tokens: 500

metrics:
  - name: "difficulty"
    version: "1.0"
    evaluators: ["lmstudio_fast"]
    parameters:
      rubric: "bloom_taxonomy"
      target_audience: "undergraduate"
    enabled: true

  - name: "coverage"
    version: "1.1"
    evaluators: ["azure_gpt4"]
    parameters:
      granularity: "balanced"
    enabled: true

  - name: "grammatical_correctness"
    version: "1.0"
    evaluators: ["lmstudio_fast"]
    enabled: true

  - name: "clarity"
    version: "1.0"
    evaluators: ["lmstudio_reasoning", "azure_gpt4"]  # Compare local vs Azure
    enabled: true

inputs:
  quiz_directory: "data/quizzes"
  source_directory: "data/inputs"

outputs:
  results_directory: "data/results"
  aggregate: true
